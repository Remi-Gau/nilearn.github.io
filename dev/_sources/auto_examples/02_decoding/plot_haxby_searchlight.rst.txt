
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/02_decoding/plot_haxby_searchlight.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_02_decoding_plot_haxby_searchlight.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_02_decoding_plot_haxby_searchlight.py:


Searchlight analysis of face vs house recognition
=================================================

Searchlight analysis requires fitting a classifier a large amount of
times. As a result, it is an intrinsically slow method. In order to speed
up computing, in this example, Searchlight is run only on one slice on
the :term:`fMRI` (see the generated figures).

.. include:: ../../../examples/masker_note.rst

.. GENERATED FROM PYTHON SOURCE LINES 15-17

Load Haxby dataset
------------------

.. GENERATED FROM PYTHON SOURCE LINES 17-33

.. code-block:: default

    import pandas as pd
    from nilearn import datasets
    from nilearn.image import get_data, load_img, new_img_like

    # We fetch 2nd subject from haxby datasets (which is default)
    haxby_dataset = datasets.fetch_haxby()

    # print basic information on the dataset
    print(f"Anatomical nifti image (3D) is located at: {haxby_dataset.mask}")
    print(f"Functional nifti image (4D) is located at: {haxby_dataset.func[0]}")

    fmri_filename = haxby_dataset.func[0]
    labels = pd.read_csv(haxby_dataset.session_target[0], sep=" ")
    y = labels["labels"]
    session = labels["chunks"]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Anatomical nifti image (3D) is located at: /home/runner/work/nilearn/nilearn/nilearn_data/haxby2001/mask.nii.gz
    Functional nifti image (4D) is located at: /home/runner/work/nilearn/nilearn/nilearn_data/haxby2001/subj2/bold.nii.gz




.. GENERATED FROM PYTHON SOURCE LINES 34-36

Restrict to faces and houses
----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 36-43

.. code-block:: default

    from nilearn.image import index_img

    condition_mask = y.isin(["face", "house"])

    fmri_img = index_img(fmri_filename, condition_mask)
    y, session = y[condition_mask], session[condition_mask]








.. GENERATED FROM PYTHON SOURCE LINES 44-50

Prepare masks
-------------
- mask_img is the original mask
- process_mask_img is a subset of mask_img, it contains the voxels that
  should be processed (we only keep the slice z = 26 and the back of the
  brain to speed up computation)

.. GENERATED FROM PYTHON SOURCE LINES 50-62

.. code-block:: default

    import numpy as np

    mask_img = load_img(haxby_dataset.mask)

    # .astype() makes a copy.
    process_mask = get_data(mask_img).astype(int)
    picked_slice = 29
    process_mask[..., (picked_slice + 1) :] = 0
    process_mask[..., :picked_slice] = 0
    process_mask[:, 30:] = 0
    process_mask_img = new_img_like(mask_img, process_mask)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/nilearn/nilearn/examples/02_decoding/plot_haxby_searchlight.py:60: UserWarning:

    Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.





.. GENERATED FROM PYTHON SOURCE LINES 63-65

Searchlight computation
-----------------------

.. GENERATED FROM PYTHON SOURCE LINES 65-92

.. code-block:: default


    # Make processing parallel
    # /!\ As each thread will print its progress, n_jobs > 1 could mess up the
    #     information output.
    n_jobs = 1

    # Define the cross-validation scheme used for validation.
    # Here we use a KFold cross-validation on the session, which corresponds to
    # splitting the samples in 4 folds and make 4 runs using each fold as a test
    # set once and the others as learning sets
    from sklearn.model_selection import KFold

    cv = KFold(n_splits=4)

    import nilearn.decoding

    # The radius is the one of the Searchlight sphere that will scan the volume
    searchlight = nilearn.decoding.SearchLight(
        mask_img,
        process_mask_img=process_mask_img,
        radius=5.6,
        n_jobs=n_jobs,
        verbose=1,
        cv=cv,
    )
    searchlight.fit(fmri_img, y)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /usr/share/miniconda3/envs/testenv/lib/python3.9/site-packages/nilearn/image/resampling.py:494: UserWarning:

    The provided image has no sform in its header. Please check the provided file. Results may not be as expected.

    [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
    Job #1, processed 0/739 voxels (0.00%, 398.3616828918457 seconds remaining)    Job #1, processed 10/739 voxels (1.35%, 33.66792893409729 seconds remaining)    Job #1, processed 20/739 voxels (2.71%, 33.89386015184691 seconds remaining)    Job #1, processed 30/739 voxels (4.06%, 34.542429823006316 seconds remaining)    Job #1, processed 40/739 voxels (5.41%, 34.253707065159205 seconds remaining)    Job #1, processed 50/739 voxels (6.77%, 34.08631240881353 seconds remaining)    Job #1, processed 60/739 voxels (8.12%, 33.811355910277726 seconds remaining)    Job #1, processed 70/739 voxels (9.47%, 33.538899326022346 seconds remaining)    Job #1, processed 80/739 voxels (10.83%, 33.22100873777191 seconds remaining)    Job #1, processed 90/739 voxels (12.18%, 32.83295631682736 seconds remaining)    Job #1, processed 100/739 voxels (13.53%, 32.504313810437324 seconds remaining)    Job #1, processed 110/739 voxels (14.88%, 32.067002870703256 seconds remaining)    Job #1, processed 120/739 voxels (16.24%, 31.653032745633812 seconds remaining)    Job #1, processed 130/739 voxels (17.59%, 31.214380862158492 seconds remaining)    Job #1, processed 140/739 voxels (18.94%, 30.78961374037117 seconds remaining)    Job #1, processed 150/739 voxels (20.30%, 30.29133172927819 seconds remaining)    Job #1, processed 160/739 voxels (21.65%, 29.847359716203968 seconds remaining)    Job #1, processed 170/739 voxels (23.00%, 29.37408206773841 seconds remaining)    Job #1, processed 180/739 voxels (24.36%, 28.89299215354356 seconds remaining)    Job #1, processed 190/739 voxels (25.71%, 28.39293568665672 seconds remaining)    Job #1, processed 200/739 voxels (27.06%, 27.92194535515525 seconds remaining)    Job #1, processed 210/739 voxels (28.42%, 27.389205419371276 seconds remaining)    Job #1, processed 220/739 voxels (29.77%, 26.909635006039984 seconds remaining)    Job #1, processed 230/739 voxels (31.12%, 26.407977589604787 seconds remaining)    Job #1, processed 240/739 voxels (32.48%, 25.90895679783939 seconds remaining)    Job #1, processed 250/739 voxels (33.83%, 25.39565465149357 seconds remaining)    Job #1, processed 260/739 voxels (35.18%, 24.90234461936712 seconds remaining)    Job #1, processed 270/739 voxels (36.54%, 24.396321530282073 seconds remaining)    Job #1, processed 280/739 voxels (37.89%, 23.88768699893413 seconds remaining)    Job #1, processed 290/739 voxels (39.24%, 23.38898275970319 seconds remaining)    Job #1, processed 300/739 voxels (40.60%, 22.867753032393054 seconds remaining)    Job #1, processed 310/739 voxels (41.95%, 22.366556243475912 seconds remaining)    Job #1, processed 320/739 voxels (43.30%, 21.853670395549397 seconds remaining)    Job #1, processed 330/739 voxels (44.65%, 21.349814555946686 seconds remaining)    Job #1, processed 340/739 voxels (46.01%, 20.825189974380045 seconds remaining)    Job #1, processed 350/739 voxels (47.36%, 20.313631727083308 seconds remaining)    Job #1, processed 360/739 voxels (48.71%, 19.80553505240072 seconds remaining)    Job #1, processed 370/739 voxels (50.07%, 19.281157883574966 seconds remaining)    Job #1, processed 380/739 voxels (51.42%, 18.770045743012695 seconds remaining)    Job #1, processed 390/739 voxels (52.77%, 18.24699348569345 seconds remaining)    Job #1, processed 400/739 voxels (54.13%, 17.727932994783682 seconds remaining)    Job #1, processed 410/739 voxels (55.48%, 17.208538571621073 seconds remaining)    Job #1, processed 420/739 voxels (56.83%, 16.74257430425312 seconds remaining)    Job #1, processed 430/739 voxels (58.19%, 16.21407971314127 seconds remaining)    Job #1, processed 440/739 voxels (59.54%, 15.696879326516365 seconds remaining)    Job #1, processed 450/739 voxels (60.89%, 15.178260177633291 seconds remaining)    Job #1, processed 460/739 voxels (62.25%, 14.64846339666221 seconds remaining)    Job #1, processed 470/739 voxels (63.60%, 14.130290671714446 seconds remaining)    Job #1, processed 480/739 voxels (64.95%, 13.606487403932766 seconds remaining)    Job #1, processed 490/739 voxels (66.31%, 13.080337549187158 seconds remaining)    Job #1, processed 500/739 voxels (67.66%, 12.554692785943574 seconds remaining)    Job #1, processed 510/739 voxels (69.01%, 12.033734849355751 seconds remaining)    Job #1, processed 520/739 voxels (70.37%, 11.50339412367342 seconds remaining)    Job #1, processed 530/739 voxels (71.72%, 10.98239963664998 seconds remaining)    Job #1, processed 540/739 voxels (73.07%, 10.455716145190333 seconds remaining)    Job #1, processed 550/739 voxels (74.42%, 9.932829528814745 seconds remaining)    Job #1, processed 560/739 voxels (75.78%, 9.405460256484417 seconds remaining)    Job #1, processed 570/739 voxels (77.13%, 8.88000432934632 seconds remaining)    Job #1, processed 580/739 voxels (78.48%, 8.357935204296906 seconds remaining)    Job #1, processed 590/739 voxels (79.84%, 7.826036903327832 seconds remaining)    Job #1, processed 600/739 voxels (81.19%, 7.303873184088754 seconds remaining)    Job #1, processed 610/739 voxels (82.54%, 6.780500023538907 seconds remaining)    Job #1, processed 620/739 voxels (83.90%, 6.253449961454277 seconds remaining)    Job #1, processed 630/739 voxels (85.25%, 5.727228531040405 seconds remaining)    Job #1, processed 640/739 voxels (86.60%, 5.204254001723153 seconds remaining)    Job #1, processed 650/739 voxels (87.96%, 4.674082872703871 seconds remaining)    Job #1, processed 660/739 voxels (89.31%, 4.149954221107405 seconds remaining)    Job #1, processed 670/739 voxels (90.66%, 3.625494474873438 seconds remaining)    Job #1, processed 680/739 voxels (92.02%, 3.096576200052855 seconds remaining)    Job #1, processed 690/739 voxels (93.37%, 2.572731716724511 seconds remaining)    Job #1, processed 700/739 voxels (94.72%, 2.0470128079523917 seconds remaining)    Job #1, processed 710/739 voxels (96.08%, 1.5183610004945964 seconds remaining)    Job #1, processed 720/739 voxels (97.43%, 0.9944237507944708 seconds remaining)    Job #1, processed 730/739 voxels (98.78%, 0.4713025701313899 seconds remaining)    [Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   38.5s finished


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SearchLight(cv=KFold(n_splits=4, random_state=None, shuffle=False),
                mask_img=&lt;nibabel.nifti1.Nifti1Image object at 0x7f6ff9a725e0&gt;,
                process_mask_img=&lt;nibabel.nifti1.Nifti1Image object at 0x7f6fff99f0a0&gt;,
                radius=5.6, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">SearchLight</label><div class="sk-toggleable__content"><pre>SearchLight(cv=KFold(n_splits=4, random_state=None, shuffle=False),
                mask_img=&lt;nibabel.nifti1.Nifti1Image object at 0x7f6ff9a725e0&gt;,
                process_mask_img=&lt;nibabel.nifti1.Nifti1Image object at 0x7f6fff99f0a0&gt;,
                radius=5.6, verbose=1)</pre></div></div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 93-95

F-scores computation
--------------------

.. GENERATED FROM PYTHON SOURCE LINES 95-114

.. code-block:: default

    from nilearn.maskers import NiftiMasker

    # For decoding, standardizing is often very important
    nifti_masker = NiftiMasker(
        mask_img=mask_img,
        runs=session,
        standardize=True,
        memory="nilearn_cache",
        memory_level=1,
    )
    fmri_masked = nifti_masker.fit_transform(fmri_img)

    from sklearn.feature_selection import f_classif

    _, p_values = f_classif(fmri_masked, y)
    p_values = -np.log10(p_values)
    p_values[p_values > 10] = 10
    p_unmasked = get_data(nifti_masker.inverse_transform(p_values))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /usr/share/miniconda3/envs/testenv/lib/python3.9/site-packages/nilearn/signal.py:915: FutureWarning:

    The default strategy for standardize is currently 'zscore' which incorrectly uses population std to calculate sample zscores. The new strategy 'zscore_sample' corrects this behavior by using the sample std. In release 0.13, the default strategy will be replaced by the new strategy and the 'zscore' option will be removed. Please use 'zscore_sample' instead.





.. GENERATED FROM PYTHON SOURCE LINES 115-118

Visualization
-------------
Use the fmri mean image as a surrogate of anatomical data

.. GENERATED FROM PYTHON SOURCE LINES 118-153

.. code-block:: default

    from nilearn import image

    mean_fmri = image.mean_img(fmri_img)

    from nilearn.plotting import plot_img, plot_stat_map, show

    searchlight_img = new_img_like(mean_fmri, searchlight.scores_)

    # Because scores are not a zero-center test statistics, we cannot use
    # plot_stat_map
    plot_img(
        searchlight_img,
        bg_img=mean_fmri,
        title="Searchlight",
        display_mode="z",
        cut_coords=[-9],
        vmin=0.42,
        cmap="hot",
        threshold=0.2,
        black_bg=True,
    )

    # F_score results
    p_ma = np.ma.array(p_unmasked, mask=np.logical_not(process_mask))
    f_score_img = new_img_like(mean_fmri, p_ma)
    plot_stat_map(
        f_score_img,
        mean_fmri,
        title="F-scores",
        display_mode="z",
        cut_coords=[-9],
        colorbar=False,
    )

    show()



.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/02_decoding/images/sphx_glr_plot_haxby_searchlight_001.png
         :alt: plot haxby searchlight
         :srcset: /auto_examples/02_decoding/images/sphx_glr_plot_haxby_searchlight_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/02_decoding/images/sphx_glr_plot_haxby_searchlight_002.png
         :alt: plot haxby searchlight
         :srcset: /auto_examples/02_decoding/images/sphx_glr_plot_haxby_searchlight_002.png
         :class: sphx-glr-multi-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  48.585 seconds)

**Estimated memory usage:**  916 MB


.. _sphx_glr_download_auto_examples_02_decoding_plot_haxby_searchlight.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/nilearn/nilearn/main?urlpath=lab/tree/notebooks/auto_examples/02_decoding/plot_haxby_searchlight.ipynb
        :alt: Launch binder
        :width: 150 px



    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_haxby_searchlight.py <plot_haxby_searchlight.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_haxby_searchlight.ipynb <plot_haxby_searchlight.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
