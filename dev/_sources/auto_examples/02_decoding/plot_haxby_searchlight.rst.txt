
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/02_decoding/plot_haxby_searchlight.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_02_decoding_plot_haxby_searchlight.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_02_decoding_plot_haxby_searchlight.py:


Searchlight analysis of face vs house recognition
=================================================

Searchlight analysis requires fitting a classifier a large amount of
times. As a result, it is an intrinsically slow method. In order to speed
up computing, in this example, Searchlight is run only on one slice on
the :term:`fMRI` (see the generated figures).

.. include:: ../../../examples/masker_note.rst

.. GENERATED FROM PYTHON SOURCE LINES 15-17

Load Haxby dataset
------------------

.. GENERATED FROM PYTHON SOURCE LINES 17-34

.. code-block:: default

    import pandas as pd

    from nilearn import datasets
    from nilearn.image import get_data, load_img, new_img_like

    # We fetch 2nd subject from haxby datasets (which is default)
    haxby_dataset = datasets.fetch_haxby()

    # print basic information on the dataset
    print(f"Anatomical nifti image (3D) is located at: {haxby_dataset.mask}")
    print(f"Functional nifti image (4D) is located at: {haxby_dataset.func[0]}")

    fmri_filename = haxby_dataset.func[0]
    labels = pd.read_csv(haxby_dataset.session_target[0], sep=" ")
    y = labels["labels"]
    session = labels["chunks"]





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Anatomical nifti image (3D) is located at: /home/runner/work/nilearn/nilearn/nilearn_data/haxby2001/mask.nii.gz
    Functional nifti image (4D) is located at: /home/runner/work/nilearn/nilearn/nilearn_data/haxby2001/subj2/bold.nii.gz




.. GENERATED FROM PYTHON SOURCE LINES 35-37

Restrict to faces and houses
----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 37-44

.. code-block:: default

    from nilearn.image import index_img

    condition_mask = y.isin(["face", "house"])

    fmri_img = index_img(fmri_filename, condition_mask)
    y, session = y[condition_mask], session[condition_mask]








.. GENERATED FROM PYTHON SOURCE LINES 45-51

Prepare masks
-------------
- mask_img is the original mask
- process_mask_img is a subset of mask_img, it contains the voxels that
  should be processed (we only keep the slice z = 29 and the back of the
  brain to speed up computation)

.. GENERATED FROM PYTHON SOURCE LINES 51-63

.. code-block:: default

    import numpy as np

    mask_img = load_img(haxby_dataset.mask)

    # .astype() makes a copy.
    process_mask = get_data(mask_img).astype(int)
    picked_slice = 29
    process_mask[..., (picked_slice + 1) :] = 0
    process_mask[..., :picked_slice] = 0
    process_mask[:, 30:] = 0
    process_mask_img = new_img_like(mask_img, process_mask)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/work/nilearn/nilearn/examples/02_decoding/plot_haxby_searchlight.py:61: UserWarning:

    Data array used to create a new image contains 64-bit ints. This is likely due to creating the array with numpy and passing `int` as the `dtype`. Many tools such as FSL and SPM cannot deal with int64 in Nifti images, so for compatibility the data has been converted to int32.





.. GENERATED FROM PYTHON SOURCE LINES 64-66

Searchlight computation
-----------------------

.. GENERATED FROM PYTHON SOURCE LINES 66-93

.. code-block:: default


    # Make processing parallel
    # /!\ As each thread will print its progress, n_jobs > 1 could mess up the
    #     information output.
    n_jobs = 1

    # Define the cross-validation scheme used for validation.
    # Here we use a KFold cross-validation on the session, which corresponds to
    # splitting the samples in 4 folds and make 4 runs using each fold as a test
    # set once and the others as learning sets
    from sklearn.model_selection import KFold

    cv = KFold(n_splits=4)

    import nilearn.decoding

    # The radius is the one of the Searchlight sphere that will scan the volume
    searchlight = nilearn.decoding.SearchLight(
        mask_img,
        process_mask_img=process_mask_img,
        radius=5.6,
        n_jobs=n_jobs,
        verbose=1,
        cv=cv,
    )
    searchlight.fit(fmri_img, y)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /usr/share/miniconda3/envs/testenv/lib/python3.9/site-packages/nilearn/image/resampling.py:491: UserWarning:

    The provided image has no sform in its header. Please check the provided file. Results may not be as expected.

    Job #1, processed 0/739 voxels (0.00%, 401.2799263000488 seconds remaining)    Job #1, processed 10/739 voxels (1.35%, 33.19735485536081 seconds remaining)    Job #1, processed 20/739 voxels (2.71%, 33.85734611419734 seconds remaining)    Job #1, processed 30/739 voxels (4.06%, 35.008882291211286 seconds remaining)    Job #1, processed 40/739 voxels (5.41%, 34.88805643423648 seconds remaining)    Job #1, processed 50/739 voxels (6.77%, 34.90676971728552 seconds remaining)    Job #1, processed 60/739 voxels (8.12%, 34.749815956125126 seconds remaining)    Job #1, processed 70/739 voxels (9.47%, 34.56582526898812 seconds remaining)    Job #1, processed 80/739 voxels (10.83%, 34.35771033792505 seconds remaining)    Job #1, processed 90/739 voxels (12.18%, 34.00638231189772 seconds remaining)    Job #1, processed 100/739 voxels (13.53%, 33.718040213969054 seconds remaining)    Job #1, processed 110/739 voxels (14.88%, 33.27960114837975 seconds remaining)    Job #1, processed 120/739 voxels (16.24%, 32.8971489521083 seconds remaining)    Job #1, processed 130/739 voxels (17.59%, 32.45362371894159 seconds remaining)    Job #1, processed 140/739 voxels (18.94%, 32.02999010332786 seconds remaining)    Job #1, processed 150/739 voxels (20.30%, 31.496837648852118 seconds remaining)    Job #1, processed 160/739 voxels (21.65%, 31.04598250730484 seconds remaining)    Job #1, processed 170/739 voxels (23.00%, 30.552449796510782 seconds remaining)    Job #1, processed 180/739 voxels (24.36%, 30.05859070066944 seconds remaining)    Job #1, processed 190/739 voxels (25.71%, 29.54638507124145 seconds remaining)    Job #1, processed 200/739 voxels (27.06%, 29.068180766708306 seconds remaining)    Job #1, processed 210/739 voxels (28.42%, 28.515031909707726 seconds remaining)    Job #1, processed 220/739 voxels (29.77%, 28.004812918535848 seconds remaining)    Job #1, processed 230/739 voxels (31.12%, 27.464870565041785 seconds remaining)    Job #1, processed 240/739 voxels (32.48%, 26.93829201007712 seconds remaining)    Job #1, processed 250/739 voxels (33.83%, 26.393294448934295 seconds remaining)    Job #1, processed 260/739 voxels (35.18%, 25.880513463391708 seconds remaining)    Job #1, processed 270/739 voxels (36.54%, 25.353420558234156 seconds remaining)    Job #1, processed 280/739 voxels (37.89%, 24.81487552104697 seconds remaining)    Job #1, processed 290/739 voxels (39.24%, 24.298927729039868 seconds remaining)    Job #1, processed 300/739 voxels (40.60%, 23.742821012224468 seconds remaining)    Job #1, processed 310/739 voxels (41.95%, 23.360845342722495 seconds remaining)    Job #1, processed 320/739 voxels (43.30%, 22.82377940843067 seconds remaining)    Job #1, processed 330/739 voxels (44.65%, 22.29941823207518 seconds remaining)    Job #1, processed 340/739 voxels (46.01%, 21.747046212117795 seconds remaining)    Job #1, processed 350/739 voxels (47.36%, 21.21736877995568 seconds remaining)    Job #1, processed 360/739 voxels (48.71%, 20.690110702432417 seconds remaining)    Job #1, processed 370/739 voxels (50.07%, 20.136042297826883 seconds remaining)    Job #1, processed 380/739 voxels (51.42%, 19.59862183495191 seconds remaining)    Job #1, processed 390/739 voxels (52.77%, 19.047456095926638 seconds remaining)    Job #1, processed 400/739 voxels (54.13%, 18.502028485531948 seconds remaining)    Job #1, processed 410/739 voxels (55.48%, 17.953384044794362 seconds remaining)    Job #1, processed 420/739 voxels (56.83%, 17.41546163272841 seconds remaining)    Job #1, processed 430/739 voxels (58.19%, 16.864564595711684 seconds remaining)    Job #1, processed 440/739 voxels (59.54%, 16.32339879743277 seconds remaining)    Job #1, processed 450/739 voxels (60.89%, 15.781306538759614 seconds remaining)    Job #1, processed 460/739 voxels (62.25%, 15.22925927744333 seconds remaining)    Job #1, processed 470/739 voxels (63.60%, 14.686213287917322 seconds remaining)    Job #1, processed 480/739 voxels (64.95%, 14.136837870456146 seconds remaining)    Job #1, processed 490/739 voxels (66.31%, 13.588506980964166 seconds remaining)    Job #1, processed 500/739 voxels (67.66%, 13.03851392478447 seconds remaining)    Job #1, processed 510/739 voxels (69.01%, 12.495073016079209 seconds remaining)    Job #1, processed 520/739 voxels (70.37%, 11.93926395545955 seconds remaining)    Job #1, processed 530/739 voxels (71.72%, 11.396280671389357 seconds remaining)    Job #1, processed 540/739 voxels (73.07%, 10.845702486487184 seconds remaining)    Job #1, processed 550/739 voxels (74.42%, 10.301002262933572 seconds remaining)    Job #1, processed 560/739 voxels (75.78%, 9.752450892024822 seconds remaining)    Job #1, processed 570/739 voxels (77.13%, 9.205112529298638 seconds remaining)    Job #1, processed 580/739 voxels (78.48%, 8.662412017128151 seconds remaining)    Job #1, processed 590/739 voxels (79.84%, 8.10908485844522 seconds remaining)    Job #1, processed 600/739 voxels (81.19%, 7.568494860123702 seconds remaining)    Job #1, processed 610/739 voxels (82.54%, 7.021758424129441 seconds remaining)    Job #1, processed 620/739 voxels (83.90%, 6.475515992195299 seconds remaining)    Job #1, processed 630/739 voxels (85.25%, 5.929427759388675 seconds remaining)    Job #1, processed 640/739 voxels (86.60%, 5.388278244274045 seconds remaining)    Job #1, processed 650/739 voxels (87.96%, 4.838554781965369 seconds remaining)    Job #1, processed 660/739 voxels (89.31%, 4.2950994540750465 seconds remaining)    Job #1, processed 670/739 voxels (90.66%, 3.752014916437026 seconds remaining)    Job #1, processed 680/739 voxels (92.02%, 3.2037980314493977 seconds remaining)    Job #1, processed 690/739 voxels (93.37%, 2.6616960836833155 seconds remaining)    Job #1, processed 700/739 voxels (94.72%, 2.1170718295348663 seconds remaining)    Job #1, processed 710/739 voxels (96.08%, 1.5700323061581762 seconds remaining)    Job #1, processed 720/739 voxels (97.43%, 1.0280838416707472 seconds remaining)    Job #1, processed 730/739 voxels (98.78%, 0.48706732150585796 seconds remaining)

.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>SearchLight(cv=KFold(n_splits=4, random_state=None, shuffle=False),
                mask_img=&lt;nibabel.nifti1.Nifti1Image object at 0x7f69f8929b20&gt;,
                process_mask_img=&lt;nibabel.nifti1.Nifti1Image object at 0x7f6a03f7e550&gt;,
                radius=5.6, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">SearchLight</label><div class="sk-toggleable__content"><pre>SearchLight(cv=KFold(n_splits=4, random_state=None, shuffle=False),
                mask_img=&lt;nibabel.nifti1.Nifti1Image object at 0x7f69f8929b20&gt;,
                process_mask_img=&lt;nibabel.nifti1.Nifti1Image object at 0x7f6a03f7e550&gt;,
                radius=5.6, verbose=1)</pre></div></div></div></div></div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 94-96

F-scores computation
--------------------

.. GENERATED FROM PYTHON SOURCE LINES 96-115

.. code-block:: default

    from nilearn.maskers import NiftiMasker

    # For decoding, standardizing is often very important
    nifti_masker = NiftiMasker(
        mask_img=mask_img,
        runs=session,
        standardize="zscore_sample",
        memory="nilearn_cache",
        memory_level=1,
    )
    fmri_masked = nifti_masker.fit_transform(fmri_img)

    from sklearn.feature_selection import f_classif

    _, p_values = f_classif(fmri_masked, y)
    p_values = -np.log10(p_values)
    p_values[p_values > 10] = 10
    p_unmasked = get_data(nifti_masker.inverse_transform(p_values))








.. GENERATED FROM PYTHON SOURCE LINES 116-119

Visualization
-------------
Use the :term:`fMRI` mean image as a surrogate of anatomical data

.. GENERATED FROM PYTHON SOURCE LINES 119-155

.. code-block:: default

    from nilearn import image

    mean_fmri = image.mean_img(fmri_img)

    from nilearn.plotting import plot_img, plot_stat_map, show

    searchlight_img = new_img_like(mean_fmri, searchlight.scores_)

    # Because scores are not a zero-center test statistics, we cannot use
    # plot_stat_map
    plot_img(
        searchlight_img,
        bg_img=mean_fmri,
        title="Searchlight",
        display_mode="z",
        cut_coords=[-9],
        vmin=0.42,
        cmap="hot",
        threshold=0.2,
        black_bg=True,
    )

    # F_score results
    p_ma = np.ma.array(p_unmasked, mask=np.logical_not(process_mask))
    f_score_img = new_img_like(mean_fmri, p_ma)
    plot_stat_map(
        f_score_img,
        mean_fmri,
        title="F-scores",
        display_mode="z",
        cut_coords=[-9],
        colorbar=False,
    )

    show()




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/02_decoding/images/sphx_glr_plot_haxby_searchlight_001.png
         :alt: plot haxby searchlight
         :srcset: /auto_examples/02_decoding/images/sphx_glr_plot_haxby_searchlight_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/02_decoding/images/sphx_glr_plot_haxby_searchlight_002.png
         :alt: plot haxby searchlight
         :srcset: /auto_examples/02_decoding/images/sphx_glr_plot_haxby_searchlight_002.png
         :class: sphx-glr-multi-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 50.534 seconds)

**Estimated memory usage:**  916 MB


.. _sphx_glr_download_auto_examples_02_decoding_plot_haxby_searchlight.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/nilearn/nilearn/main?urlpath=lab/tree/notebooks/auto_examples/02_decoding/plot_haxby_searchlight.ipynb
        :alt: Launch binder
        :width: 150 px



    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_haxby_searchlight.py <plot_haxby_searchlight.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_haxby_searchlight.ipynb <plot_haxby_searchlight.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
