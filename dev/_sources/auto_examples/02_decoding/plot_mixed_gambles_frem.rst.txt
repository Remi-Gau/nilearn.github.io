
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/02_decoding/plot_mixed_gambles_frem.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_02_decoding_plot_mixed_gambles_frem.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_02_decoding_plot_mixed_gambles_frem.py:


FREM on Jimura et al "mixed gambles" dataset
============================================

In this example, we use fast ensembling of regularized models (FREM) to
solve a regression problem, predicting the gain level corresponding to each
:term:`beta<Beta>` maps regressed from mixed gambles experiment.
:term:`FREM` uses an implicit spatial regularization through fast clustering
and aggregates a high number
of  estimators trained on various splits of the training set, thus returning
a very robust decoder at a lower computational cost than other spatially
regularized methods.

To have more details, see: :ref:`frem`.

.. GENERATED FROM PYTHON SOURCE LINES 18-20

Load the data from the Jimura mixed-gamble experiment
-----------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 20-28

.. code-block:: Python

    from nilearn.datasets import fetch_mixed_gambles

    data = fetch_mixed_gambles(n_subjects=16)

    zmap_filenames = data.zmaps
    behavioral_target = data.gain
    mask_filename = data.mask_img





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Dataset created in /home/runner/work/nilearn/nilearn/nilearn_data/jimura_poldrack_2012_zmaps

    Downloading data from https://www.nitrc.org/frs/download.php/7229/jimura_poldrack_2012_zmaps.zip ...
    Downloaded 1875968 of 104293434 bytes (1.8%,   54.6s remaining)    Downloaded 3301376 of 104293434 bytes (3.2%,  1.0min remaining)    Downloaded 4702208 of 104293434 bytes (4.5%,  1.1min remaining)    Downloaded 6209536 of 104293434 bytes (6.0%,  1.1min remaining)    Downloaded 7897088 of 104293434 bytes (7.6%,  1.0min remaining)    Downloaded 9797632 of 104293434 bytes (9.4%,   58.8s remaining)    Downloaded 11829248 of 104293434 bytes (11.3%,   55.5s remaining)    Downloaded 14049280 of 104293434 bytes (13.5%,   52.2s remaining)    Downloaded 16474112 of 104293434 bytes (15.8%,   48.8s remaining)    Downloaded 18522112 of 104293434 bytes (17.8%,   47.0s remaining)    Downloaded 20783104 of 104293434 bytes (19.9%,   44.9s remaining)    Downloaded 23216128 of 104293434 bytes (22.3%,   42.7s remaining)    Downloaded 25698304 of 104293434 bytes (24.6%,   40.4s remaining)    Downloaded 28303360 of 104293434 bytes (27.1%,   38.2s remaining)    Downloaded 30195712 of 104293434 bytes (29.0%,   37.5s remaining)    Downloaded 31744000 of 104293434 bytes (30.4%,   37.3s remaining)    Downloaded 33366016 of 104293434 bytes (32.0%,   36.9s remaining)    Downloaded 35151872 of 104293434 bytes (33.7%,   36.1s remaining)    Downloaded 37150720 of 104293434 bytes (35.6%,   35.0s remaining)    Downloaded 39305216 of 104293434 bytes (37.7%,   33.7s remaining)    Downloaded 41590784 of 104293434 bytes (39.9%,   32.3s remaining)    Downloaded 44146688 of 104293434 bytes (42.3%,   30.5s remaining)    Downloaded 46858240 of 104293434 bytes (44.9%,   28.8s remaining)    Downloaded 49537024 of 104293434 bytes (47.5%,   27.0s remaining)    Downloaded 52363264 of 104293434 bytes (50.2%,   25.3s remaining)    Downloaded 55484416 of 104293434 bytes (53.2%,   23.3s remaining)    Downloaded 58654720 of 104293434 bytes (56.2%,   21.4s remaining)    Downloaded 61997056 of 104293434 bytes (59.4%,   19.5s remaining)    Downloaded 65306624 of 104293434 bytes (62.6%,   17.7s remaining)    Downloaded 68698112 of 104293434 bytes (65.9%,   15.9s remaining)    Downloaded 72245248 of 104293434 bytes (69.3%,   14.0s remaining)    Downloaded 75464704 of 104293434 bytes (72.4%,   12.5s remaining)    Downloaded 78430208 of 104293434 bytes (75.2%,   11.1s remaining)    Downloaded 81018880 of 104293434 bytes (77.7%,   10.0s remaining)    Downloaded 83648512 of 104293434 bytes (80.2%,    8.8s remaining)    Downloaded 86392832 of 104293434 bytes (82.8%,    7.6s remaining)    Downloaded 89300992 of 104293434 bytes (85.6%,    6.3s remaining)    Downloaded 92323840 of 104293434 bytes (88.5%,    5.0s remaining)    Downloaded 95485952 of 104293434 bytes (91.6%,    3.7s remaining)    Downloaded 98959360 of 104293434 bytes (94.9%,    2.2s remaining)    Downloaded 102547456 of 104293434 bytes (98.3%,    0.7s remaining) ...done. (43 seconds, 0 min)
    Extracting data from /home/runner/work/nilearn/nilearn/nilearn_data/jimura_poldrack_2012_zmaps/a4c8868ab5c651b8594da6f3204ded3a/jimura_poldrack_2012_zmaps.zip..... done.




.. GENERATED FROM PYTHON SOURCE LINES 29-33

Fit FREM
--------
We compare both of these models to a pipeline ensembling many models


.. GENERATED FROM PYTHON SOURCE LINES 33-52

.. code-block:: Python

    from nilearn.decoding import FREMRegressor

    frem = FREMRegressor("svr", cv=10, standardize="zscore_sample")

    frem.fit(zmap_filenames, behavioral_target)

    # Visualize FREM weights
    # ----------------------

    from nilearn.plotting import plot_stat_map

    plot_stat_map(
        frem.coef_img_["beta"],
        title="FREM",
        display_mode="yz",
        cut_coords=[20, -2],
        threshold=0.2,
    )




.. image-sg:: /auto_examples/02_decoding/images/sphx_glr_plot_mixed_gambles_frem_001.png
   :alt: plot mixed gambles frem
   :srcset: /auto_examples/02_decoding/images/sphx_glr_plot_mixed_gambles_frem_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/nilearn/decoding/decoder.py:742: UserWarning:

    Brain mask is bigger than the volume of a standard human brain. This object is probably not tuned to be used on such data.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.

    /opt/hostedtoolcache/Python/3.12.2/x64/lib/python3.12/site-packages/sklearn/svm/_base.py:297: ConvergenceWarning:

    Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.


    <nilearn.plotting.displays._slicers.YZSlicer object at 0x7fe573b880b0>



.. GENERATED FROM PYTHON SOURCE LINES 53-61

We can observe that the coefficients map learnt
by :term:`FREM` is structured,
due to the spatial regularity imposed by working on clusters and model
ensembling. Although these maps have been thresholded for display, they are
not sparse (i.e. almost allÂ voxels have non-zero coefficients). See also this
:ref:`other example <sphx_glr_auto_examples_02_decoding_plot_haxby_frem.py>`
using FREM, and related :ref:`section of user guide <frem>`.


.. GENERATED FROM PYTHON SOURCE LINES 63-72

Example use of TV-L1 SpaceNet
-----------------------------
:ref:`SpaceNet<space_net>` is another method available in Nilearn to decode
with spatially sparse models. Depending on the penalty that is used,
it yields either very structured maps (TV-L1) or unstructured maps
(graph_net). Because of their heavy computational costs, these methods are
not demonstrated on this example but you can try them easily if you have a
few minutes. Example code is included below.


.. GENERATED FROM PYTHON SOURCE LINES 72-89

.. code-block:: Python


    from nilearn.decoding import SpaceNetRegressor

    # We use the regressor object since the task is to predict a continuous
    # variable (gain of the gamble).

    tv_l1 = SpaceNetRegressor(
        mask=mask_filename,
        penalty="tv-l1",
        eps=1e-1,  # prefer large alphas
        memory="nilearn_cache",
        n_jobs=2,
    )
    # tv_l1.fit(zmap_filenames, behavioral_target)
    # plot_stat_map(tv_l1.coef_img_, title="TV-L1", display_mode="yz",
    #               cut_coords=[20, -2])









.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (1 minutes 8.742 seconds)

**Estimated memory usage:**  2138 MB


.. _sphx_glr_download_auto_examples_02_decoding_plot_mixed_gambles_frem.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/nilearn/nilearn/main?urlpath=lab/tree/notebooks/auto_examples/02_decoding/plot_mixed_gambles_frem.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_mixed_gambles_frem.ipynb <plot_mixed_gambles_frem.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_mixed_gambles_frem.py <plot_mixed_gambles_frem.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
