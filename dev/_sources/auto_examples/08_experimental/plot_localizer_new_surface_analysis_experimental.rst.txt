
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/08_experimental/plot_localizer_new_surface_analysis_experimental.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_08_experimental_plot_localizer_new_surface_analysis_experimental.py>`
        to download the full example code. or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_08_experimental_plot_localizer_new_surface_analysis_experimental.py:


Example of surface-based first-level analysis
=============================================

.. warning::

    This example is adapted from
    :ref:`sphx_glr_auto_examples_04_glm_first_level_plot_localizer_surface_analysis.py`. # noqa
    to show how to use the new tentative API for surface images in nilearn.

    This functionality is provided
    by the :mod:`nilearn.experimental.surface` module.

    It is still incomplete and subject to change without a deprecation cycle.

    Please participate in the discussion on GitHub!

A full step-by-step example of fitting a :term:`GLM`
to experimental data sampled on the cortical surface
and visualizing the results.

More specifically:

1. A sequence of :term:`fMRI` volumes is loaded.
2. :term:`fMRI` data are projected onto a reference cortical surface
   (the FreeSurfer template, fsaverage).
3. A :term:`GLM` is applied to the dataset
   (effect/covariance, then contrast estimation).

The result of the analysis are statistical maps that are defined
on the brain mesh.
We display them using Nilearn capabilities.

The projection of :term:`fMRI` data onto a given brain :term:`mesh` requires
that both are initially defined in the same space.

* The functional data should be coregistered to the anatomy
  from which the mesh was obtained.

* Another possibility, used here, is to project
  the normalized :term:`fMRI` data to an :term:`MNI`-coregistered mesh,
  such as fsaverage.

The advantage of this second approach is that it makes it easy to run
second-level analyses on the surface.
On the other hand, it is obviously less accurate
than using a subject-tailored mesh.

.. GENERATED FROM PYTHON SOURCE LINES 52-56

Prepare data and analysis parameters
------------------------------------

Prepare the timing parameters.

.. GENERATED FROM PYTHON SOURCE LINES 56-59

.. code-block:: Python

    t_r = 2.4
    slice_time_ref = 0.5








.. GENERATED FROM PYTHON SOURCE LINES 60-62

Prepare the data.
First, the volume-based :term:`fMRI` data.

.. GENERATED FROM PYTHON SOURCE LINES 62-67

.. code-block:: Python

    from nilearn import datasets

    data = datasets.fetch_localizer_first_level()
    fmri_img = data.epi_img








.. GENERATED FROM PYTHON SOURCE LINES 68-69

Second, the experimental paradigm.

.. GENERATED FROM PYTHON SOURCE LINES 69-74

.. code-block:: Python

    import pandas as pd

    events_file = data.events
    events = pd.read_table(events_file)








.. GENERATED FROM PYTHON SOURCE LINES 75-83

Project the :term:`fMRI` image to the surface
---------------------------------------------

For this we need to get a :term:`mesh`
representing the geometry of the surface.
We could use an individual :term:`mesh`,
but we first resort to a standard :term:`mesh`,
the so-called fsaverage5 template from the FreeSurfer software.

.. GENERATED FROM PYTHON SOURCE LINES 86-91

We use the new :class:`nilearn.experimental.surface.SurfaceImage`
to create an surface object instance
that contains both the mesh
(here we use the one from the fsaverage5 templates)
and the BOLD data that we project on the surface.

.. GENERATED FROM PYTHON SOURCE LINES 91-107

.. code-block:: Python

    from nilearn import surface
    from nilearn.experimental.surface import SurfaceImage, load_fsaverage

    fsaverage5 = load_fsaverage()
    texture_left = surface.vol_to_surf(fmri_img, fsaverage5["pial"].parts["left"])
    texture_right = surface.vol_to_surf(
        fmri_img, fsaverage5["pial"].parts["right"]
    )
    image = SurfaceImage(
        mesh=fsaverage5["pial"],
        data={
            "left": texture_left.T,
            "right": texture_right.T,
        },
    )








.. GENERATED FROM PYTHON SOURCE LINES 108-118

Perform first level analysis
----------------------------

We can now simply run a GLM by directly passing
our :class:`nilearn.experimental.surface.SurfaceImage` instance
as input to FirstLevelModel.fit

Here we use an :term:`HRF` model
containing the Glover model and its time derivative
The drift model is implicitly a cosine basis with a period cutoff at 128s.

.. GENERATED FROM PYTHON SOURCE LINES 118-126

.. code-block:: Python

    from nilearn.glm.first_level import FirstLevelModel

    glm = FirstLevelModel(
        t_r,
        slice_time_ref=slice_time_ref,
        hrf_model="glover + derivative",
    ).fit(image, events)








.. GENERATED FROM PYTHON SOURCE LINES 127-134

Estimate contrasts
------------------

Specify the contrasts.

For practical purpose, we first generate an identity matrix whose size is
the number of columns of the design matrix.

.. GENERATED FROM PYTHON SOURCE LINES 134-139

.. code-block:: Python

    import numpy as np

    design_matrix = glm.design_matrices_[0]
    contrast_matrix = np.eye(design_matrix.shape[1])








.. GENERATED FROM PYTHON SOURCE LINES 140-141

At first, we create basic contrasts.

.. GENERATED FROM PYTHON SOURCE LINES 141-146

.. code-block:: Python

    basic_contrasts = {
        column: contrast_matrix[i]
        for i, column in enumerate(design_matrix.columns)
    }








.. GENERATED FROM PYTHON SOURCE LINES 147-149

Next, we add some intermediate contrasts and
one :term:`contrast` adding all conditions with some auditory parts.

.. GENERATED FROM PYTHON SOURCE LINES 149-175

.. code-block:: Python

    basic_contrasts["audio"] = (
        basic_contrasts["audio_left_hand_button_press"]
        + basic_contrasts["audio_right_hand_button_press"]
        + basic_contrasts["audio_computation"]
        + basic_contrasts["sentence_listening"]
    )

    # one contrast adding all conditions involving instructions reading
    basic_contrasts["visual"] = (
        basic_contrasts["visual_left_hand_button_press"]
        + basic_contrasts["visual_right_hand_button_press"]
        + basic_contrasts["visual_computation"]
        + basic_contrasts["sentence_reading"]
    )

    # one contrast adding all conditions involving computation
    basic_contrasts["computation"] = (
        basic_contrasts["visual_computation"]
        + basic_contrasts["audio_computation"]
    )

    # one contrast adding all conditions involving sentences
    basic_contrasts["sentences"] = (
        basic_contrasts["sentence_listening"] + basic_contrasts["sentence_reading"]
    )








.. GENERATED FROM PYTHON SOURCE LINES 176-188

Finally, we create a dictionary of more relevant contrasts

* 'left - right button press': probes motor activity
  in left versus right button presses.
* 'audio - visual': probes the difference of activity between listening
  to some content or reading the same type of content
  (instructions, stories).
* 'computation - sentences': looks at the activity
  when performing a mental computation task  versus simply reading sentences.

Of course, we could define other contrasts,
but we keep only 3 for simplicity.

.. GENERATED FROM PYTHON SOURCE LINES 188-206

.. code-block:: Python


    contrasts = {
        "(left - right) button press": (
            basic_contrasts["audio_left_hand_button_press"]
            - basic_contrasts["audio_right_hand_button_press"]
            + basic_contrasts["visual_left_hand_button_press"]
            - basic_contrasts["visual_right_hand_button_press"]
        ),
        "audio - visual": basic_contrasts["audio"] - basic_contrasts["visual"],
        "computation - sentences": (
            basic_contrasts["computation"] - basic_contrasts["sentences"]
        ),
    }


    from nilearn.experimental.plotting import plot_surf_stat_map
    from nilearn.experimental.surface import load_fsaverage_data








.. GENERATED FROM PYTHON SOURCE LINES 207-208

Let's estimate the contrasts by iterating over them.

.. GENERATED FROM PYTHON SOURCE LINES 208-235

.. code-block:: Python

    from nilearn.plotting import show

    fsaverage_data = load_fsaverage_data(data_type="sulcal")

    for index, (contrast_id, contrast_val) in enumerate(contrasts.items()):
        # compute contrast-related statistics
        z_score = glm.compute_contrast(contrast_val, stat_type="t")

        # we plot it on the surface, on the inflated fsaverage mesh,
        # together with a suitable background to give an impression
        # of the cortex folding.
        for hemi in ["left", "right"]:
            print(
                f"  Contrast {index + 1:1} out of {len(contrasts)}: "
                f"{contrast_id}, {hemi} hemisphere"
            )
            plot_surf_stat_map(
                surf_mesh=fsaverage5["inflated"],
                stat_map=z_score,
                hemi=hemi,
                title=contrast_id,
                colorbar=True,
                threshold=3.0,
                bg_map=fsaverage_data,
            )

    show()



.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_001.png
         :alt: (left - right) button press
         :srcset: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_002.png
         :alt: (left - right) button press
         :srcset: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_002.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_003.png
         :alt: audio - visual
         :srcset: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_003.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_004.png
         :alt: audio - visual
         :srcset: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_004.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_005.png
         :alt: computation - sentences
         :srcset: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_005.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_006.png
         :alt: computation - sentences
         :srcset: /auto_examples/08_experimental/images/sphx_glr_plot_localizer_new_surface_analysis_experimental_006.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

      Contrast 1 out of 3: (left - right) button press, left hemisphere
      Contrast 1 out of 3: (left - right) button press, right hemisphere
      Contrast 2 out of 3: audio - visual, left hemisphere
      Contrast 2 out of 3: audio - visual, right hemisphere
      Contrast 3 out of 3: computation - sentences, left hemisphere
      Contrast 3 out of 3: computation - sentences, right hemisphere





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 10.174 seconds)

**Estimated memory usage:**  210 MB


.. _sphx_glr_download_auto_examples_08_experimental_plot_localizer_new_surface_analysis_experimental.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: binder-badge

      .. image:: images/binder_badge_logo.svg
        :target: https://mybinder.org/v2/gh/nilearn/nilearn/main?urlpath=lab/tree/notebooks/auto_examples/08_experimental/plot_localizer_new_surface_analysis_experimental.ipynb
        :alt: Launch binder
        :width: 150 px

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_localizer_new_surface_analysis_experimental.ipynb <plot_localizer_new_surface_analysis_experimental.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_localizer_new_surface_analysis_experimental.py <plot_localizer_new_surface_analysis_experimental.py>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
